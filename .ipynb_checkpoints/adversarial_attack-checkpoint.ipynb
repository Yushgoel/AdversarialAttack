{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import mnist, cifar10, cifar100\n",
    "\n",
    "from tensorflow.keras import Sequential, Input, Model\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten, Activation\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "#different classes model has to learn to differentiate\n",
    "#code is written this way to make it generalizable to other datasets\n",
    "labels = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes (10000, 28, 28, 1) (10000, 10) (60000, 28, 28, 1) (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "#defining characteristics of the dataset\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "num_classes = 10\n",
    "\n",
    "#Normalizing Data\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "#Reshaping data into required format\n",
    "x_train = x_train.reshape((-1, img_rows, img_cols, channels))\n",
    "x_test = x_test.reshape((-1, img_rows, img_cols, channels))\n",
    "\n",
    "#One hot encoding labels into a more NN friendly format\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(\"Data shapes\", x_test.shape, y_test.shape, x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following model architecture is the baseline model I will be using. The model has been pre-trained to save time in another notebook called MNIST_baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 0.9818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0029318640008568764, 0.9818000197410583]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def create_baseline_model():\n",
    "    X_Input = Input((img_rows, img_cols, channels))\n",
    "    X = Conv2D(32, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu')(X_Input)\n",
    "    X = Conv2D(64, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu')(X)\n",
    "    X = Conv2D(64, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(32)(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(32)(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    \n",
    "    model = Model(inputs = X_Input, outputs = X)\n",
    "    return model\n",
    "\n",
    "model = create_baseline_model()\n",
    "\n",
    "model.load_weights(filepath='./baseline_model.hdf5')\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "# Check if model is imported correctly. Accuracy should be 0.981800019\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the next code cell implements the adversarial attack algorithm. What this does essentially is finds the loss function of the model with respect to the image, and instead of updating the image to minimize loss, the model updates the image to maximize loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_pattern(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        prediction = model(image)\n",
    "        loss = tf.keras.losses.MSE(label, prediction)\n",
    "    \n",
    "    gradient = tape.gradient(loss, image)\n",
    "    \n",
    "    signed_grad = tf.sign(gradient)\n",
    "    \n",
    "    return signed_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will generate adversarial examples! Right now, I will only generate adversarial examples for the first 3000 rows of the training set, but this is purely to save time. We will still be able to see a huge drop in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "x_adversarial = np.zeros((3000, 28, 28, 1))\n",
    "\n",
    "for i in range(3000):\n",
    "    image = x_train[i]\n",
    "    image_label = y_train[i]\n",
    "    \n",
    "    perturbations = adversarial_pattern(image.reshape((1, img_rows, img_cols, channels)), image_label).numpy()\n",
    "    adversarial = image + perturbations * 0.1\n",
    "    x_adversarial[i, :, :, :] = adversarial\n",
    "    preds.append(model.predict(adversarial).argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see how are model performed on the adversarial examples. We will also compare it to how it performs on regular examples. There is a drop from 98% to 23%, which is almost 75%!. Here we see the need to prevent such attacks, as in a few minutes, we were able to bring the model down on its knees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1367 - accuracy: 0.2293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13667051494121552, 0.2293333262205124]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see the accuracy of model against adversarial\n",
    "model.evaluate(x_adversarial[0: 3000, :, :, :], y_train[0 : 3000, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 0.9890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.001836622366681695, 0.9890000224113464]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see the accuracy of model against regular data to compare. The above value should be very low compared to this one.\n",
    "model.evaluate(x_train[0: 3000, :, :, :], y_train[0 : 3000, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on Adversarial Data\n",
    "The first approach we will try is the standard approach of training on adversarial data. I will not be using pure adversarial data as I don't want the model to forget how to classify regular images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_train = np.concatenate((x_adversarial, x_train[0 : 3000, :, :, :]))\n",
    "ad_y_train = np.concatenate((y_train[0 : 3000, :],y_train[0 : 3000, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 28, 28, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train the model. When I was running experiments, I found something very interesting: if the epochs are set to 20, and then I try to implement adversarial attacks on the test set and evaluate, the model actually performs worse than otherwise. This means that the model is overfitting to adversarial examples, and hence is only memorizing some examples. Although overfitting happens for regular data as well, it is important to note that when training the model on regular data, the model didn't overfit even after 20 epochs, and the test accuracy was still going up. \n",
    "\n",
    "Conclusion: Adversarial Examples are extremely easy to overfit to. Hence, it won't be extremely effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 0.0276 - accuracy: 0.8448\n",
      "Epoch 2/2\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 0.0141 - accuracy: 0.9203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f807404eeb8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(ad_train, ad_y_train, batch_size=32, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0131 - accuracy: 0.9267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.013093532994389534, 0.9266666769981384]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_adversarial[0: 3000, :, :, :], y_train[0 : 3000, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "test_adversarial = np.zeros((3000, 28, 28, 1))\n",
    "\n",
    "for i in range(3000):\n",
    "    image = x_test[i]\n",
    "    image_label = y_test[i]\n",
    "    \n",
    "    perturbations = adversarial_pattern(image.reshape((1, img_rows, img_cols, channels)), image_label).numpy()\n",
    "    adversarial = image + perturbations * 0.1\n",
    "    test_adversarial[i, :, :, :] = adversarial\n",
    "    test_preds.append(model.predict(adversarial).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 28, 28, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_adversarial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0775 - accuracy: 0.5860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07750704884529114, 0.5860000252723694]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#So accuracy has increased to 61% from 20%. Major Improvement! But that's still not enough. Still 40% of the time, the model is horribly wrong.\n",
    "model.evaluate(test_adversarial, y_test[0 : 3000, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0050 - accuracy: 0.9727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.004953921306878328, 0.9726999998092651]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Model Weights\n",
    "model.save(\"adv_trained_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good thing is that after training on the adversarial examples, the model has grown a tolerance towards them, and improves to 61%. However, we are stuck here now, since we can't train the model more, else it will overfit the training data and perform worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 920us/step - loss: 0.0029 - accuracy: 0.9818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0029318640008568764, 0.9818000197410583]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_baseline_model()\n",
    "\n",
    "model.load_weights(filepath='./baseline_model.hdf5')\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "# Check if model is imported correctly. Accuracy should be 0.981800019\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_noise_preds = []\n",
    "noise_adversarial = np.zeros((3000, 28, 28, 1))\n",
    "\n",
    "for i in range(3000):\n",
    "    image = x_test[i]\n",
    "    image_label = y_test[i]\n",
    "    \n",
    "    perturbations = adversarial_pattern(image.reshape((1, img_rows, img_cols, channels)), image_label).numpy()\n",
    "    adversarial = image + perturbations * 0.1\n",
    "    adversarial = adversarial>0.3\n",
    "    noise_adversarial[i, :, :, :] = adversarial\n",
    "    anti_noise_preds.append(model.predict(adversarial).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0146 - accuracy: 0.9140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.014571293257176876, 0.9139999747276306]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(noise_adversarial, y_test[0 : 3000, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like denoising is amazing! It increases the accuracy to 91%. That's almost perfect - but now let's see how it holds against increased noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_noise_preds = []\n",
    "noise_adversarial = np.zeros((3000, 28, 28, 1))\n",
    "no_noise_adversarial = np.zeros((3000, 28, 28, 1))\n",
    "\n",
    "for i in range(3000):\n",
    "    image = x_test[i]\n",
    "    image_label = y_test[i]\n",
    "    \n",
    "    perturbations = adversarial_pattern(image.reshape((1, img_rows, img_cols, channels)), image_label).numpy()\n",
    "    adversarial = image + perturbations * 0.3\n",
    "    no_noise_adversarial[i, :, :, :] = adversarial\n",
    "    adversarial = adversarial>0.3\n",
    "    noise_adversarial[i, :, :, :] = adversarial\n",
    "    anti_noise_preds.append(model.predict(adversarial).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f80742d3630>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(no_noise_adversarial[0, :, :, :].reshape((img_rows, img_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8073408d68>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(noise_adversarial[0, :, :, :].reshape((img_rows, img_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1768 - accuracy: 0.0473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17678509652614594, 0.047333333641290665]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(noise_adversarial, y_test[0 : 3000, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 5 percent. That's horrible - increasing the noise by a little completely ruined the model's performance. Hence, although this binary thresholding is the best approach we currently have, it won't be good enough since a small change can ruin it. Now we see the need for a more robust system. As I have shown, using current techniques doesn't help much since it isn't robust.\n",
    "Firstly, I will be trying out a filtering technique, which aims to identify and exclude adversarial examples from even enterring the model. The first filtering technique which was tried above was denoising, but we can see that this isn't particularly robust to different amounts of noise. Let's train a NN to predict whether or not a datapoint is adversarial. Then we can decline potentially harmful examples.\n",
    "\n",
    "First thing to do is to generate the training and testing data for the model. I will try out different noise levels for the test dataset to see if the model generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_train = np.zeros((3000, 28, 28, 1))\n",
    "\n",
    "adv_test = np.zeros((3000, 28, 28, 1))\n",
    "y_adv_train = np.zeros((6000, 1))\n",
    "y_adv_test = np.zeros((6000, 1))\n",
    "\n",
    "for i in range(3000):\n",
    "    image = x_train[i]\n",
    "    image_label = y_train[i]\n",
    "    \n",
    "    perturbations = adversarial_pattern(image.reshape((1, img_rows, img_cols, channels)), image_label).numpy()\n",
    "    \n",
    "    image_test = x_test[i]\n",
    "    image_label_test = y_test[i]\n",
    "    \n",
    "    perturbations_test = adversarial_pattern(image_test.reshape((1, img_rows, img_cols, channels)), image_label_test).numpy()\n",
    "\n",
    "    if i > 1499:\n",
    "        adversarial = image + perturbations * 0.1\n",
    "        adv_train[i] = adversarial\n",
    "        \n",
    "        adversarial = image_test + perturbations_test * 0.1\n",
    "        adv_test[i] = adversarial\n",
    "        \n",
    "    else:\n",
    "        adversarial = image + perturbations * 0.3\n",
    "        adv_train[i] = adversarial\n",
    "        \n",
    "        adversarial = image_test + perturbations_test * 0.4\n",
    "        adv_test[i] = adversarial\n",
    "    \n",
    "    y_adv_train[i] = 1\n",
    "    y_adv_test[i] = 1\n",
    "    \n",
    "fin_adv_train = np.concatenate((adv_train, x_train[3000 : 6000, :, :, :]))\n",
    "\n",
    "fin_adv_test = np.concatenate((adv_test, x_test[3000 : 6000, :, :, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of final data are \n",
      "((6000, 28, 28, 1), (6000, 1), (6000, 28, 28, 1), (6000, 1))\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapes of final data are \")\n",
    "print((fin_adv_train.shape, y_adv_train.shape, fin_adv_test.shape, y_adv_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8073b17048>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAOm0lEQVR4nO3de4xc5XnH8d/Pd/ClsTFQ11gBgylFpEC0MTSQlBaVglUCNE0ELZGRaB1VUAVC1aA0UmhaKU6VBCUqSmMHB4cQEK1xcVXSQlxSSktdlkvAxAETaoKxYwMmMZTGl/XTP/Y4WmDPO+u528/3I61m5jzzznk02t+e2XnnzOuIEIBD37heNwCgOwg7kARhB5Ig7EAShB1IYkI3dzbJk2OKpnZzl0AqP9P/anfs8mi1lsJu+3xJX5I0XtLXImJp6f5TNFVn+NxWdgmgYF2sra01/TLe9nhJN0m6QNLJki6zfXKzjwegs1r5n32hpGcj4rmI2C3pDkkXtactAO3WStjnSnphxO3N1bY3sb3E9qDtwT3a1cLuALSilbCP9ibA2z57GxHLImIgIgYmanILuwPQilbCvlnSvBG3j5G0pbV2AHRKK2F/WNIC28fZniTpUklr2tMWgHZreuotIvbavlrSv2h46m1FRDzVts4AtFVL8+wRcY+ke9rUC4AO4uOyQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSRaWrLZ9iZJr0kakrQ3Igba0RSA9msp7JXfiIiX2/A4ADqIl/FAEq2GPSTda/sR20tGu4PtJbYHbQ/u0a4WdwegWa2+jD8rIrbYPkrSfbZ/EBEPjLxDRCyTtEySZnhWtLg/AE1q6cgeEVuqy+2SVkta2I6mALRf02G3PdX29P3XJZ0naX27GgPQXq28jD9a0mrb+x/nWxHxz23pCl0zYf6xxfqWRb9UrE+/cGuxfv8pqw60pZ8b7/KxaCj2FesnP3hFbe34T71efuyNzxXrB6Omwx4Rz0k6tY29AOggpt6AJAg7kARhB5Ig7EAShB1Ioh0nwqDHxk2fXlt76dJTimMvv+bbxfqfvOOupnra7x/fmFFb2763vjYWU7y7WP/+2bfU1k794NXFsXOXHnpTbxzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJ5tkPAh4oz5Xv/Ks3amvr3nVTcez/RXmu+tR1VxbrRy47vFg/7OEf1taGXtlRHNvIrkXvKdb/YPlXa2tHnvti+cGXNtNRf+PIDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM/eB17/8JnF+mc+u7xYP2fKntraLTvLXwX91aWXFOtzVz5UrDcy1NLoMl+7vemx2747t1ifp+ebfux+xZEdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JwRHRtZzM8K87wuV3bX7+I95YXu/3MN1cU6++Z7GJ9wao/rq2d9KkNxbFDO3cW6/3sfU/8rFgfivpj2UMD04pjY0/5PP9+tS7WamfsGPUXpuGR3fYK29ttrx+xbZbt+2xvrC5ntrNhAO03lpfxt0g6/y3brpe0NiIWSFpb3QbQxxqGPSIekPTW7w+6SNLK6vpKSRe3uS8AbdbsG3RHR8RWSaouj6q7o+0ltgdtD+7RriZ3B6BVHX83PiKWRcRARAxM1ORO7w5AjWbDvs32HEmqLps//QhAVzQb9jWSFlfXF0u6uz3tAOiUhuez275d0jmSZtveLOnTGv5W7TttXynpR5I+1MkmD3ZTPrutWG80j/7BZy8o1hdcM1hbG9rXyTPKe2vV/5xWrH/z1K/X1r7xuWuLY0/4+H811VM/axj2iLisppTv0zHAQYyPywJJEHYgCcIOJEHYgSQIO5AEXyXdBi8v+bVi/Tvzv1Csv7qvfJrxlhXzi/WZ+8pTe4eqn7xSPk31pIn1n9i84H2PFcdubKqj/saRHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJ59jDyh/qlasPjp4tgZ46YU6yfddlWxPr/FZZMPVi9+4r3F+g9++0sNHmF8beWhr727OHK2Dr3nnCM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBPPsYPf3l+nnZZ4/92+LYMx+7tFg/4YbvFev7itWD17jp04v1X/1AebnpCYV5dEm68Jnfqa3NXv7fxbGHIo7sQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE8+xjdOHC8veMl/jvjijW973xTNOP3e88cVJtbffqmcWxtx77Dy3t++U3ptbWZh7CS1nXaXhkt73C9nbb60dsu8H2i7Yfr34WdbZNAK0ay8v4WySdP8r2GyPitOrnnva2BaDdGoY9Ih6QtKMLvQDooFbeoLva9hPVy/zaf75sL7E9aHtwj3a1sDsArWg27F+RdLyk0yRtlVS7cmFELIuIgYgYmKj6hfYAdFZTYY+IbRExFBH7JC2XtLC9bQFot6bCbnvOiJuXSFpfd18A/aHhPLvt2yWdI2m27c2SPi3pHNunSQpJmyR9tIM9dsWE495ZrF9+xN8XquXzqo/8zvPF+t5itb+NnzGjWN+z+hdqa/eeVJ5HH+/ysWgoymf6v7S9vrfyDP+hqWHYI+KyUTbf3IFeAHQQH5cFkiDsQBKEHUiCsANJEHYgCU5x3W/3nmL5x3vrp5A06fXi2KevKU/r/fKXy39z976wuVgv8eQGn1o85YRi+ZkrphXrf3leaUpS+r1pP66tnfRvf1Qce//Zf1OszxpXf/qsJB2zhl/vkTiyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASTERW9r64pVi/btXi2tpvXn5jcezTv39Tsf7d351YrN/9av1y0Y3MmPBGsf4XR97a9GNL0vah8uMv/Pyf1dZO+NdXi2P3nV3e959ufX+xfvjqdeUHSIYjO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwTz7GM2//qHa2q8/d21x7Mevu7NYv3TaS8X6OXOany/+yKZzi/UTHllSrM9aV/4MwNF3PFWs/+LO/6ytvfJPC4pj544/vFj/9mPvKtZP1MPFejYc2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCebZ22D2svo5eEm67c5TivVvHXZYO9t5k6GXXi7WT9w72NrjN6iPmzKltvaH8/+jpX0fd2d5yWa8WcMju+15tu+3vcH2U7Y/Vm2fZfs+2xury4xLXgMHjbG8jN8r6bqI+BVJZ0q6yvbJkq6XtDYiFkhaW90G0Kcahj0itkbEo9X11yRtkDRX0kWSVlZ3Wynp4k41CaB1B/QGne1jJZ0uaZ2koyNiqzT8B0HSUTVjltgetD24R7ta6xZA08YcdtvTJK2SdE1E7BzruIhYFhEDETEwUQ0WGQTQMWMKu+2JGg76bRFxV7V5m+05VX2OpO2daRFAOzScerNtSTdL2hARXxxRWiNpsaSl1eXdHenwEDD0k5+W79CofhDbdsXptbUrZ9Sf/ipJy386r1ifvO6ZYp2JuTcbyzz7WZI+IulJ249X2z6p4ZDfaftKST+S9KHOtAigHRqGPSIelOSacvmbEQD0DT4uCyRB2IEkCDuQBGEHkiDsQBKc4oqOmvSB8tdkl3zuwUXF+omv8VXRB4IjO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwTw7OmrhUc83PXby1vJy0TgwHNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgiYZhtz3P9v22N9h+yvbHqu032H7R9uPVT/lLvgH01Fi+vGKvpOsi4lHb0yU9Yvu+qnZjRHy+c+0BaJexrM++VdLW6vprtjdImtvpxgC01wH9z277WEmnS1pXbbra9hO2V9ieWTNmie1B24N7tKulZgE0b8xhtz1N0ipJ10TETklfkXS8pNM0fOT/wmjjImJZRAxExMBETW5DywCaMaaw256o4aDfFhF3SVJEbIuIoYjYJ2m5pIWdaxNAq8bybrwl3SxpQ0R8ccT2OSPudomk9e1vD0C7OCLKd7DPlvTvkp6UtK/a/ElJl2n4JXxI2iTpo9WbebVmeFac4XNbbBlAnXWxVjtjh0erjeXd+AcljTb4nlYbA9A9fIIOSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQRMPz2du6M/slSc+P2DRb0stda+DA9Gtv/dqXRG/Namdv74yII0crdDXsb9u5PRgRAz1roKBfe+vXviR6a1a3euNlPJAEYQeS6HXYl/V4/yX92lu/9iXRW7O60ltP/2cH0D29PrID6BLCDiTRk7DbPt/207aftX19L3qoY3uT7SerZagHe9zLCtvbba8fsW2W7ftsb6wuR11jr0e99cUy3oVlxnv63PV6+fOu/89ue7ykZyT9lqTNkh6WdFlEfL+rjdSwvUnSQET0/AMYtt8v6XVJ34iIU6ptfy1pR0Qsrf5QzoyIT/RJbzdIer3Xy3hXqxXNGbnMuKSLJV2hHj53hb4+rC48b704si+U9GxEPBcRuyXdIemiHvTR9yLiAUk73rL5Ikkrq+srNfzL0nU1vfWFiNgaEY9W11+TtH+Z8Z4+d4W+uqIXYZ8r6YURtzerv9Z7D0n32n7E9pJeNzOKo/cvs1VdHtXjft6q4TLe3fSWZcb75rlrZvnzVvUi7KMtJdVP839nRcS7JV0g6arq5SrGZkzLeHfLKMuM94Vmlz9vVS/CvlnSvBG3j5G0pQd9jCoitlSX2yWtVv8tRb1t/wq61eX2Hvfzc/20jPdoy4yrD567Xi5/3ouwPyxpge3jbE+SdKmkNT3o421sT63eOJHtqZLOU/8tRb1G0uLq+mJJd/ewlzfpl2W865YZV4+fu54vfx4RXf+RtEjD78j/UNKf96KHmr7mS/pe9fNUr3uTdLuGX9bt0fAroislHSFpraSN1eWsPurtVg0v7f2EhoM1p0e9na3hfw2fkPR49bOo189doa+uPG98XBZIgk/QAUkQdiAJwg4kQdiBJAg7kARhB5Ig7EAS/w9kokTpEDfOXgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#checking to see if data is as desired.\n",
    "plt.imshow(fin_adv_train[3000, :, :, :].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_classifier():\n",
    "    X_Input = Input((img_rows, img_cols, channels))\n",
    "    X = Conv2D(32, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu')(X_Input)\n",
    "    X = Conv2D(64, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(32)(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(1, activation='sigmoid')(X)\n",
    "    \n",
    "    adv_model = Model(inputs = X_Input, outputs = X)\n",
    "    return adv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 10, 10, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 27,073\n",
      "Trainable params: 27,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "adv_model = adv_classifier()\n",
    "adv_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "adv_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 0.1655 - accuracy: 0.9345\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 3.7465e-04 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 1.8127e-04 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 7.9648e-05 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 7.0082e-05 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 5.0723e-05 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 3.0002e-05 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 2.6075e-05 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 2.0114e-05 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = adv_model.fit(fin_adv_train, y_adv_train, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 0s 2ms/step - loss: 8.4749e-06 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.47487444843864e-06, 1.0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_model.evaluate(fin_adv_test, y_adv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = adv_model.predict(fin_adv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000000e+00],\n",
       "       [1.0000000e+00],\n",
       "       [1.0000000e+00],\n",
       "       ...,\n",
       "       [1.3129377e-06],\n",
       "       [2.1377024e-05],\n",
       "       [6.4337117e-05]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0861 - accuracy: 0.5338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08614670485258102, 0.5338333249092102]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#before adding model safecatch\n",
    "model.evaluate(fin_adv_test, y_test[0 : 6000, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9796666666666667\n"
     ]
    }
   ],
   "source": [
    "#adding model safecatch\n",
    "model_safe = adv_model.predict(fin_adv_test)\n",
    "correct = 0\n",
    "counter = 0\n",
    "for i in range(len(model_safe)):\n",
    "    if model_safe[i] > 0.5:\n",
    "        pass\n",
    "    else:\n",
    "        mod_pred = model.predict(fin_adv_test[i, :, :, :].reshape(-1, 28, 28, 1)).argmax()\n",
    "        if mod_pred == y_test[i].argmax():\n",
    "            correct += 1\n",
    "        counter += 1\n",
    "print('accuracy = ' + str(correct / counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model's accuracy has gone back up. The reason for this is that our model for filtering adversarial examples was very good. The problem is that we now have to run 2 models instead of 1, and in many applications, this is too slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting foolbox==3.0.0b1\n",
      "  Downloading foolbox-3.0.0b1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/yush/anaconda3/lib/python3.7/site-packages (from foolbox==3.0.0b1) (1.18.5)\n",
      "Collecting typing-extensions>=3.7.4.1\n",
      "  Downloading typing_extensions-3.7.4.2-py3-none-any.whl (22 kB)\n",
      "Collecting eagerpy==0.25.2\n",
      "  Downloading eagerpy-0.25.2-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: scipy in /Users/yush/anaconda3/lib/python3.7/site-packages (from foolbox==3.0.0b1) (1.4.1)\n",
      "Requirement already satisfied: setuptools in /Users/yush/anaconda3/lib/python3.7/site-packages (from foolbox==3.0.0b1) (41.0.1)\n",
      "Collecting GitPython>=3.0.7\n",
      "  Downloading GitPython-3.1.3-py3-none-any.whl (451 kB)\n",
      "\u001b[K     |████████████████████████████████| 451 kB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.5-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 2.3 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
      "  Downloading smmap-3.0.4-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: typing-extensions, eagerpy, smmap, gitdb, GitPython, foolbox\n",
      "Successfully installed GitPython-3.1.3 eagerpy-0.25.2 foolbox-3.0.0b1 gitdb-4.0.5 smmap-3.0.4 typing-extensions-3.7.4.2\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/Users/yush/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install foolbox==3.0.0b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import foolbox as fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/yush/anaconda3/lib/python3.7/site-packages/foolbox/models/tensorflow.py:13: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    }
   ],
   "source": [
    "preprocessing = dict()\n",
    "bounds = (0, 1)\n",
    "fmodel = fb.TensorFlowModel(model, bounds=bounds, preprocessing=preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = fb.utils.samples(fmodel, dataset='mnist', batchsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.949999988079071"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb.utils.accuracy(fmodel, images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attack = fb.attacks.LinfDeepFoolAttack()\n",
    "attack = fb.attacks.LinfPGD()\n",
    "epsilons = [0.0, 0.001, 0.01, 0.03, 0.1, 0.3, 0.5, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw, clipped, is_adv = attack(fmodel, images, labels, epsilons=-1)\n",
    "_, advs, success = attack(fmodel, images, labels, epsilons=epsilons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 20), dtype=bool, numpy=\n",
       "array([[False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "         True, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "         True, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "         True, False],\n",
       "       [False, False, False, False, False, False,  True, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "         True, False],\n",
       "       [ True, False,  True,  True,  True, False,  True,  True,  True,\n",
       "         True, False,  True,  True,  True, False, False, False,  True,\n",
       "         True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True]])>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "advss = np.array(advs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7f3941cf98>"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(advss[4, 0, :, :, :].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "advsss = advss.reshape(20*8, 28, 28, 1)\n",
    "ad_labls = np.zeros((160,))\n",
    "ad_labls[80 : 160] = np.ones((80,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 4.7090 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.709028720855713, 0.5]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_model.evaluate(advsss, ad_labls) \n",
    "#Extremely bad filtering model against different type of attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 28, 28, 1)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_np = raw.numpy()\n",
    "raw_np.shape\n",
    "imagess = images.numpy()\n",
    "labelss = tf.keras.utils.to_categorical(labels.numpy(), num_classes)\n",
    "count = 0\n",
    "for i in range(20):\n",
    "    pred = model.predict(raw_np[i, :, :, :].reshape(-1, 28, 28, 1)).argmax()\n",
    "    if pred == labels[i]:\n",
    "        count += 1\n",
    "print(\"Accuracy is \" + str(count / 20))\n",
    "is_adv_try = is_adv.numpy().astype('int')\n",
    "adv_model.evaluate(raw_np, np.ones((20,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this experiment, we can see that this filtering model is robust to different noise levels, but not very robust to different methods of attacks. We can see that against another gradient based attack, our filtering model was only able to filter out 80% of images. The other 20% still got through, meaning that a hacker may have to now try 4-5 attacks for atleast 1 to work. However, this is also not safe enough, since the model (once the image passes the filter), is guaranteed to get it wrong. \n",
    "\n",
    "Although we can use this as a filtering step when our application permits, we need a better approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will try to normalize all of the inputs to the layers of the neural network. Maybe, by standardizing inputs, we can reduce the impact a single pixel has, and hopefully make such models less suceptible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding BatchNormalization After every input\n",
    "def create_batchnorm_model():\n",
    "    X_Input = Input((img_rows, img_cols, channels))\n",
    "    X = Conv2D(32, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu')(X_Input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Conv2D(64, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Conv2D(64, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(32)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(32)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    \n",
    "    model = Model(inputs = X_Input, outputs = X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 10, 10, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 10, 10, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 1, 1, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 60,106\n",
      "Trainable params: 59,658\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_model = create_batchnorm_model()\n",
    "batch_model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "batch_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0160 - accuracy: 0.8938 - val_loss: 0.0046 - val_accuracy: 0.9701\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0068 - accuracy: 0.9565 - val_loss: 0.0037 - val_accuracy: 0.9758\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0055 - accuracy: 0.9641 - val_loss: 0.0032 - val_accuracy: 0.9794\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0048 - accuracy: 0.9690 - val_loss: 0.0032 - val_accuracy: 0.9789\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0042 - accuracy: 0.9726 - val_loss: 0.0030 - val_accuracy: 0.9811\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0039 - accuracy: 0.9748 - val_loss: 0.0029 - val_accuracy: 0.9807\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0035 - accuracy: 0.9776 - val_loss: 0.0028 - val_accuracy: 0.9825\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0034 - accuracy: 0.9776 - val_loss: 0.0026 - val_accuracy: 0.9838\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0031 - accuracy: 0.9803 - val_loss: 0.0026 - val_accuracy: 0.9830\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0030 - accuracy: 0.9804 - val_loss: 0.0024 - val_accuracy: 0.9843\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0028 - accuracy: 0.9823 - val_loss: 0.0023 - val_accuracy: 0.9845\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0027 - accuracy: 0.9830 - val_loss: 0.0024 - val_accuracy: 0.9848\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0025 - accuracy: 0.9844 - val_loss: 0.0023 - val_accuracy: 0.9851\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0023 - accuracy: 0.9853 - val_loss: 0.0023 - val_accuracy: 0.9849\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0024 - accuracy: 0.9849 - val_loss: 0.0023 - val_accuracy: 0.9852\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0023 - accuracy: 0.9855 - val_loss: 0.0025 - val_accuracy: 0.9843\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0021 - accuracy: 0.9869 - val_loss: 0.0023 - val_accuracy: 0.9851\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0021 - accuracy: 0.9865 - val_loss: 0.0025 - val_accuracy: 0.9840\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0019 - accuracy: 0.9873 - val_loss: 0.0023 - val_accuracy: 0.9859\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0020 - accuracy: 0.9871 - val_loss: 0.0021 - val_accuracy: 0.9872\n"
     ]
    }
   ],
   "source": [
    "history = batch_model.fit(x_train, y_train, batch_size=32, epochs=20, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model's Accuracy is \n",
      "313/313 [==============================] - 0s 970us/step - loss: 0.0029 - accuracy: 0.9818\n",
      "\n",
      "Batch Normalization Model's accuracy is \n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.002098589204251766, 0.9872000217437744]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's just double check to make sure we haven't decreased model accuracy - Generally, this shouldn't happen, although the large number of Batch Normalization layers might have an impact.\n",
    "print(\"Original Model's Accuracy is \")\n",
    "model.evaluate(x_test, y_test)\n",
    "print()\n",
    "print(\"Batch Normalization Model's accuracy is \")\n",
    "batch_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good thing is that Batch Norm has not decreased model accuracy, rather slightly increased it - although it is a small change and won't help much in performance.\n",
    "Now let's repeat the exercise to fool this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_pattern_m(model_t, image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        prediction = model_t(image)\n",
    "        loss = tf.keras.losses.MSE(label, prediction)\n",
    "    \n",
    "    gradient = tape.gradient(loss, image)\n",
    "    \n",
    "    signed_grad = tf.sign(gradient)\n",
    "    \n",
    "    return signed_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "x_adversarial = np.zeros((3000, 28, 28, 1))\n",
    "\n",
    "for i in range(3000):\n",
    "    image = x_train[i]\n",
    "    image_label = y_train[i]\n",
    "    \n",
    "    perturbations = adversarial_pattern_m(batch_model, image.reshape((1, img_rows, img_cols, channels)), image_label).numpy()\n",
    "    adversarial = image + perturbations * 0.1\n",
    "    x_adversarial[i, :, :, :] = adversarial\n",
    "    preds.append(batch_model.predict(adversarial).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1863 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18634691834449768, 0.0]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's hope that the model is more robust\n",
    "batch_model.evaluate(x_adversarial[0: 3000, :, :, :], y_train[0 : 3000, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Model Weights\n",
    "batch_model.save(\"batch_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that Batch Normalization actually hurts the model. The reason for this must be that the model is used to inputs being normalized appropriately using BatchNorm, but adversarial examples are not normalized properly and so do worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's see if increasing the dropout a lot helps in reducing the dependence on each neuron, and makes the models more robust. I will be increasing dropout by 3 - 4 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Increasing Dropout\n",
    "def dropout_inc_model():\n",
    "    X_Input = Input((img_rows, img_cols, channels))\n",
    "    X = Conv2D(32, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu')(X_Input)\n",
    "    X = Conv2D(64, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu')(X)\n",
    "    X = Conv2D(64, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.8)(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(32)(X)\n",
    "    X = Dropout(0.6)(X)\n",
    "    X = Dense(32)(X)\n",
    "    X = Dropout(0.4)(X)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    \n",
    "    model = Model(inputs = X_Input, outputs = X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 10, 10, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 59,210\n",
      "Trainable params: 59,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "d_model = dropout_inc_model()\n",
    "d_model.compile('adam', loss='mse', metrics=['accuracy'])\n",
    "d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0643 - accuracy: 0.4692 - val_loss: 0.0285 - val_accuracy: 0.8567\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0481 - accuracy: 0.6383 - val_loss: 0.0182 - val_accuracy: 0.9051\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0436 - accuracy: 0.6799 - val_loss: 0.0150 - val_accuracy: 0.9158\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0406 - accuracy: 0.7053 - val_loss: 0.0126 - val_accuracy: 0.9294\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0385 - accuracy: 0.7241 - val_loss: 0.0115 - val_accuracy: 0.9300\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0375 - accuracy: 0.7354 - val_loss: 0.0110 - val_accuracy: 0.9347\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0356 - accuracy: 0.7509 - val_loss: 0.0102 - val_accuracy: 0.9382\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0353 - accuracy: 0.7552 - val_loss: 0.0107 - val_accuracy: 0.9338\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0339 - accuracy: 0.7687 - val_loss: 0.0104 - val_accuracy: 0.9370\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0333 - accuracy: 0.7735 - val_loss: 0.0097 - val_accuracy: 0.9417\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0322 - accuracy: 0.7844 - val_loss: 0.0094 - val_accuracy: 0.9428\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0318 - accuracy: 0.7886 - val_loss: 0.0104 - val_accuracy: 0.9382\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0312 - accuracy: 0.7933 - val_loss: 0.0103 - val_accuracy: 0.9406\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0306 - accuracy: 0.7987 - val_loss: 0.0107 - val_accuracy: 0.9366\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0301 - accuracy: 0.8025 - val_loss: 0.0107 - val_accuracy: 0.9380\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0297 - accuracy: 0.8065 - val_loss: 0.0094 - val_accuracy: 0.9446\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0292 - accuracy: 0.8098 - val_loss: 0.0098 - val_accuracy: 0.9415\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0295 - accuracy: 0.8083 - val_loss: 0.0105 - val_accuracy: 0.9402\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0292 - accuracy: 0.8103 - val_loss: 0.0096 - val_accuracy: 0.9428\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0286 - accuracy: 0.8145 - val_loss: 0.0100 - val_accuracy: 0.9413\n"
     ]
    }
   ],
   "source": [
    "history = d_model.fit(x_train, y_train, batch_size=32, epochs=20, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "x_adversarial = np.zeros((3000, 28, 28, 1))\n",
    "\n",
    "for i in range(3000):\n",
    "    image = x_train[i]\n",
    "    image_label = y_train[i]\n",
    "    \n",
    "    perturbations = adversarial_pattern_m(d_model, image.reshape((1, img_rows, img_cols, channels)), image_label).numpy()\n",
    "    adversarial = image + perturbations * 0.1\n",
    "    x_adversarial[i, :, :, :] = adversarial\n",
    "    preds.append(d_model.predict(adversarial).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0240 - accuracy: 0.8497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.023957913741469383, 0.8496666550636292]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model.evaluate(x_adversarial[0: 3000, :, :, :], y_train[0:3000, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out that adding high amounts of dropout didn't help at all. This might be because of the small model, but this will generalize to larget models as well, in that increasing dropout may help performance a little, but not enough to make them secure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_test_model():\n",
    "    X_Input = Input((img_rows, img_cols, channels))\n",
    "    X = Conv2D(32, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu')(X_Input)\n",
    "    X = Dropout(0.4)(X, training=True)   #Extra block added\n",
    "    X = Conv2D(64, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu')(X)\n",
    "    X = Conv2D(64, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.4)(X, training=True)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(32)(X)\n",
    "    X = Dropout(0.2)(X, training=True)\n",
    "    X = Dense(32)(X)\n",
    "    X = Dropout(0.05)(X, training=True)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    \n",
    "    model = Model(inputs = X_Input, outputs = X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 10, 10, 32)        320       \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 59,210\n",
      "Trainable params: 59,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "d_model = dropout_test_model()\n",
    "d_model.compile('adam', loss='mse', metrics=['accuracy'])\n",
    "d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0262 - accuracy: 0.8164 - val_loss: 0.0163 - val_accuracy: 0.8916\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0139 - accuracy: 0.9082 - val_loss: 0.0123 - val_accuracy: 0.9198\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0117 - accuracy: 0.9241 - val_loss: 0.0108 - val_accuracy: 0.9304\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0105 - accuracy: 0.9323 - val_loss: 0.0097 - val_accuracy: 0.9383\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0097 - accuracy: 0.9374 - val_loss: 0.0092 - val_accuracy: 0.9411\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0092 - accuracy: 0.9413 - val_loss: 0.0087 - val_accuracy: 0.9460\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0087 - accuracy: 0.9447 - val_loss: 0.0089 - val_accuracy: 0.9418\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0084 - accuracy: 0.9462 - val_loss: 0.0084 - val_accuracy: 0.9462\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0081 - accuracy: 0.9481 - val_loss: 0.0081 - val_accuracy: 0.9474\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0079 - accuracy: 0.9496 - val_loss: 0.0079 - val_accuracy: 0.9480\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0076 - accuracy: 0.9516 - val_loss: 0.0082 - val_accuracy: 0.9480\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0076 - accuracy: 0.9519 - val_loss: 0.0075 - val_accuracy: 0.9526\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0074 - accuracy: 0.9535 - val_loss: 0.0077 - val_accuracy: 0.9515\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0073 - accuracy: 0.9538 - val_loss: 0.0071 - val_accuracy: 0.9559\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0072 - accuracy: 0.9543 - val_loss: 0.0067 - val_accuracy: 0.9582\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0070 - accuracy: 0.9557 - val_loss: 0.0076 - val_accuracy: 0.9513\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0070 - accuracy: 0.9554 - val_loss: 0.0069 - val_accuracy: 0.9571\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0068 - accuracy: 0.9575 - val_loss: 0.0072 - val_accuracy: 0.9533\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0068 - accuracy: 0.9565 - val_loss: 0.0070 - val_accuracy: 0.9573\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0067 - accuracy: 0.9581 - val_loss: 0.0067 - val_accuracy: 0.9579\n"
     ]
    }
   ],
   "source": [
    "history = d_model.fit(x_train, y_train, batch_size=32, epochs=20, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0069 - accuracy: 0.9556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.006904025096446276, 0.9556000232696533]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "x_adversarial = np.zeros((3000, 28, 28, 1))\n",
    "\n",
    "for i in range(3000):\n",
    "    image = x_train[i]\n",
    "    image_label = y_train[i]\n",
    "    \n",
    "    perturbations = adversarial_pattern_m(d_model, image.reshape((1, img_rows, img_cols, channels)), image_label).numpy()\n",
    "    adversarial = image + perturbations * 0.1\n",
    "    x_adversarial[i, :, :, :] = adversarial\n",
    "    preds.append(d_model.predict(adversarial).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0491 - accuracy: 0.6870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04910464957356453, 0.6869999766349792]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model.evaluate(x_adversarial[0: 3000, :, :, :], y_train[0:3000, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For image models, dropout in the convolutional layer is very important.\n",
    "Increasing dropout as much as possible will make the model more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous dropout score - 0 / 0.4 / 0.2 / 0.05\n",
    "0.4156666696071625\n",
    "\n",
    "Current Dropout Score - 0.4 / 0.4 / 0.2 / 0.05\n",
    "0.6869999766349792\n",
    "\n",
    "These experiments show that increasing the dropout increases score a lot.\n",
    "\n",
    "However, there is a tradeoff - Increasing dropout too much prevents the model from learning the data properly, and hence model performs poorly. Lower dropout, and adversarial attacks are more successful. \n",
    "\n",
    "Now let's run a grid search for searching all values of dropout in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_grid_search(d1, d2, d3):\n",
    "    X_Input = Input((img_rows, img_cols, channels))\n",
    "    X = Conv2D(32, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu')(X_Input)\n",
    "    X = Dropout(d1)(X, training=True)   #Extra block added\n",
    "    X = Conv2D(64, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu')(X)\n",
    "    X = Conv2D(64, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(d2)(X, training=True)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(32)(X)\n",
    "    X = Dropout(d3)(X, training=True)\n",
    "    X = Dense(32)(X)\n",
    "    X = Dropout(0.05)(X, training=True)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    \n",
    "    model = Model(inputs = X_Input, outputs = X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D1 best value = 0.45\n",
    "# D2 best value = 0.3\n",
    "# D3 best value = 0.075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0226 - accuracy: 0.8431 - val_loss: 0.0125 - val_accuracy: 0.9163\n",
      "Epoch 2/15\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0115 - accuracy: 0.9246 - val_loss: 0.0096 - val_accuracy: 0.9393\n",
      "Epoch 3/15\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0094 - accuracy: 0.9384 - val_loss: 0.0086 - val_accuracy: 0.9448\n",
      "Epoch 4/15\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0086 - accuracy: 0.9452 - val_loss: 0.0078 - val_accuracy: 0.9486\n",
      "Epoch 5/15\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0081 - accuracy: 0.9477 - val_loss: 0.0080 - val_accuracy: 0.9475\n",
      "Epoch 6/15\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0075 - accuracy: 0.9519 - val_loss: 0.0067 - val_accuracy: 0.9552\n",
      "Epoch 7/15\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0073 - accuracy: 0.9533 - val_loss: 0.0070 - val_accuracy: 0.9542\n",
      "Epoch 8/15\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0070 - accuracy: 0.9555 - val_loss: 0.0071 - val_accuracy: 0.9548\n",
      "Epoch 9/15\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0067 - accuracy: 0.9576 - val_loss: 0.0062 - val_accuracy: 0.9595\n",
      "Epoch 10/15\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0066 - accuracy: 0.9581 - val_loss: 0.0063 - val_accuracy: 0.9586\n",
      "Epoch 11/15\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0063 - accuracy: 0.9598 - val_loss: 0.0065 - val_accuracy: 0.9595\n",
      "Epoch 12/15\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0063 - accuracy: 0.9600 - val_loss: 0.0066 - val_accuracy: 0.9576\n",
      "Epoch 13/15\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0062 - accuracy: 0.9604 - val_loss: 0.0059 - val_accuracy: 0.9630\n",
      "Epoch 14/15\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0059 - accuracy: 0.9625 - val_loss: 0.0060 - val_accuracy: 0.9616\n",
      "Epoch 15/15\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0059 - accuracy: 0.9625 - val_loss: 0.0061 - val_accuracy: 0.9607\n"
     ]
    }
   ],
   "source": [
    "fin_d_model = dropout_grid_search(0.45, 0.3, 0.075)\n",
    "fin_d_model.compile('adam', loss='mse', metrics=['accuracy'])\n",
    "history = fin_d_model.fit(x_train, y_train, batch_size=32, epochs=15, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0055 - accuracy: 0.9646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.005549668334424496, 0.9646000266075134]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_d_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0315 - accuracy: 0.7937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03149035573005676, 0.793666660785675]"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_d_model.evaluate(x_adversarial[0: 3000, :, :, :], y_train[0:3000, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is excellent against adversarial examples. Let's also increase noise to 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.4267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08965112268924713, 0.4266666769981384]"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_d_model.evaluate(x_adversarial[0: 3000, :, :, :], y_train[0:3000, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving model weights\n",
    "fin_d_model.save(\"dropout_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy has gone down, quite a bit, but this is still much better than other techniques which went down to 15% or less.\n",
    "Combining this with other approaches might be extremely effective against adversarial examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "x_adversarial = np.zeros((3000, 28, 28, 1))\n",
    "\n",
    "for i in range(3000):\n",
    "    image = x_train[i]\n",
    "    image_label = y_train[i]\n",
    "    \n",
    "    perturbations = adversarial_pattern_m(fin_d_model, image.reshape((1, img_rows, img_cols, channels)), image_label).numpy()\n",
    "    adversarial = image + perturbations * 0.2\n",
    "    x_adversarial[i, :, :, :] = adversarial\n",
    "    preds.append(fin_d_model.predict(adversarial).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for dropouts   0.05\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0061 - accuracy: 0.9615\n",
      "Training for dropouts   0.1\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0063 - accuracy: 0.9598\n",
      "Training for dropouts   0.15\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0068 - accuracy: 0.9560\n",
      "Training for dropouts   0.2\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0062 - accuracy: 0.9605\n",
      "Training for dropouts   0.25\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0067 - accuracy: 0.9581\n",
      "Training for dropouts   0.3\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0072 - accuracy: 0.9541\n"
     ]
    }
   ],
   "source": [
    "#d1s = [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55]\n",
    "#d2s = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45]\n",
    "d3s = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "\n",
    "n_accuracy = []\n",
    "a_accuracy = []\n",
    "\n",
    "for d3 in d3s:\n",
    "    print(\"Training for dropouts   \" + str(d3))\n",
    "    d_model = dropout_grid_search(0.45, 0.3, d3)\n",
    "    d_model.compile('adam', loss='mse', metrics=['accuracy'])\n",
    "    history = d_model.fit(x_train, y_train, batch_size=32, epochs=15, validation_data = (x_test, y_test), verbose=0)\n",
    "\n",
    "    \n",
    "    preds = np.zeros(1000)\n",
    "    x_adversarial = np.zeros((1000, 28, 28, 1))\n",
    "    ys = np.zeros(1000)\n",
    "    for i in range(1000):\n",
    "        image = x_train[i]\n",
    "        image_label = y_train[i]\n",
    "\n",
    "        perturbations = adversarial_pattern_m(d_model, image.reshape((1, img_rows, img_cols, channels)), image_label).numpy()\n",
    "        adversarial = image + perturbations * 0.1\n",
    "        x_adversarial[i, :, :, :] = adversarial\n",
    "\n",
    "        preds[i] = d_model.predict(adversarial).argmax()\n",
    "        ys[i] = y_train[i].argmax()\n",
    "\n",
    "    a_accuracy.append((ys == preds).sum() / 1000)\n",
    "    n_accuracy.append(d_model.evaluate(x_test, y_test)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.9614999890327454,\n",
       "  0.9598000049591064,\n",
       "  0.9559999704360962,\n",
       "  0.9605000019073486,\n",
       "  0.9581000208854675,\n",
       "  0.9541000127792358],\n",
       " [0.762, 0.829, 0.57, 0.729, 0.639, 0.627])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_accuracy, a_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.7234999890327454,\n",
       " 1.7888000049591064,\n",
       " 1.525999970436096,\n",
       " 1.6895000019073487,\n",
       " 1.5971000208854675,\n",
       " 1.5811000127792358]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot = []\n",
    "for i in range(len(n_accuracy)):\n",
    "    tot.append(n_accuracy[i] + a_accuracy[i])\n",
    "tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
